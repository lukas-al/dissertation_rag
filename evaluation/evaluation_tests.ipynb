{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE TO SELF\n",
    "The bootstrapping of an edge threshold was taking too long, so I manually applied one.\n",
    "\n",
    "I could either reduce the complexity of the bootstrap calculation, or figure out the matrix multiplication that would allow me to represent an n-depth search through the graph.\n",
    "\n",
    "I reckon I should just do the former. Only calculate the average order of each node and optimise it to be 10 max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "from StructuredRag.algorithms.inquirer import StructRAGInquirer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select the runs you want to load\n",
    "# for experiment in sorted(os.listdir('../results')):\n",
    "#     print('Experiment:', experiment)\n",
    "#     for run in sorted(os.listdir('../results/' + experiment)):\n",
    "#         print(\"     || Run:\", run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate our retrievers \n",
    "Based on each of our persisted graphs\n",
    "\n",
    "Manually writing out the loop for simpler error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_cases.yml', 'r') as f:\n",
    "    test_cases = yaml.safe_load(f)\n",
    "\n",
    "for tc in test_cases['test_cases']:\n",
    "    query = tc['question']\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V0 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '/v0/2024-05-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading item: adj_matrix\n",
      "Loading item: edge_thresh\n",
      "Loading item: embedded_index\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the inquirer\n",
    "inquirer = StructRAGInquirer(\n",
    "    path_to_experiment=\"../results/\"+graph_path,\n",
    "    llm_name='google/flan-t5-large',\n",
    "    llm_max_tokens=512,\n",
    "    use_anchor_document=False # False means we compare against all the embeddings in the entire index, then find the context using the graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 3.0.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1320 > 512). Running this sequence through the model will result in indexing errors\n",
      "c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\generation\\utils.py:1283: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 3.0.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 3.0.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the answers and context\n",
    "result_holder = []\n",
    "for tc in test_cases[\"test_cases\"]:\n",
    "    query = tc[\"question\"]\n",
    "\n",
    "    res = inquirer.run_inquirer(\n",
    "        query=query,\n",
    "        source_document_name=None,\n",
    "        k_context=5,\n",
    "    )\n",
    "\n",
    "    result_holder.append(res)\n",
    "\n",
    "# Save the results\n",
    "total_results[\"v0\"] = result_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '/v1/2024-05-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading item: adj_matrix\n",
      "Loading item: embedded_index\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'edge_thresh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mn:\\CECD\\10. Personal\\Lukas Alemu\\Study Repository\\99. Capstone\\dissertation_rag\\evaluation\\evaluation_tests.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Instantiate the inquirer\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m inquirer \u001b[39m=\u001b[39m StructRAGInquirer(\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     path_to_experiment\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../results/\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m+\u001b[39;49mgraph_path,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     llm_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgoogle/flan-t5-large\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     llm_max_tokens\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     use_anchor_document\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m \u001b[39m# False means we compare against all the embeddings in the entire index, then find the context using the graph\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/evaluation/evaluation_tests.ipynb#X36sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "File \u001b[1;32m\\\\MAFP-NWSRV\\DATA\\CECD\\10. Personal\\Lukas Alemu\\Study Repository\\99. Capstone\\dissertation_rag\\src\\StructuredRag\\algorithms\\inquirer.py:39\u001b[0m, in \u001b[0;36mStructRAGInquirer.__init__\u001b[1;34m(self, path_to_experiment, llm_name, llm_max_tokens, use_anchor_document)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Instantiate the class variables and llm\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedded_index \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39membedded_index\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_thresh \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39medge_thresh\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madj_matrix \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39madj_matrix\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm \u001b[39m=\u001b[39m HuggingFacePipeline\u001b[39m.\u001b[39mfrom_model_id(\n\u001b[0;32m     42\u001b[0m     model_id\u001b[39m=\u001b[39mllm_name, \n\u001b[0;32m     43\u001b[0m     task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext2text-generation\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     },\n\u001b[0;32m     47\u001b[0m )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'edge_thresh'"
     ]
    }
   ],
   "source": [
    "# Instantiate the inquirer\n",
    "inquirer = StructRAGInquirer(\n",
    "    path_to_experiment=\"../results/\"+graph_path,\n",
    "    llm_name='google/flan-t5-large',\n",
    "    llm_max_tokens=512,\n",
    "    use_anchor_document=False # False means we compare against all the embeddings in the entire index, then find the context using the graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answers and context\n",
    "result_holder = []\n",
    "for tc in test_cases[\"test_cases\"]:\n",
    "    query = tc[\"question\"]\n",
    "\n",
    "    res = inquirer.run_inquirer(\n",
    "        query=query,\n",
    "        source_document_name=None,\n",
    "        k_context=5,\n",
    "    )\n",
    "\n",
    "    result_holder.append(res)\n",
    "\n",
    "# Save the results\n",
    "total_results[\"v1\"] = result_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '/v3/2024-05-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the inquirer\n",
    "inquirer = StructRAGInquirer(\n",
    "    path_to_experiment=\"../results/\"+graph_path,\n",
    "    llm_name='google/flan-t5-large',\n",
    "    llm_max_tokens=512,\n",
    "    use_anchor_document=False # False means we compare against all the embeddings in the entire index, then find the context using the graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answers and context\n",
    "result_holder = []\n",
    "for tc in test_cases[\"test_cases\"]:\n",
    "    query = tc[\"question\"]\n",
    "\n",
    "    res = inquirer.run_inquirer(\n",
    "        query=query,\n",
    "        source_document_name=None,\n",
    "        k_context=5,\n",
    "    )\n",
    "\n",
    "    result_holder.append(res)\n",
    "\n",
    "# Save the results\n",
    "total_results[\"v3\"] = result_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '/v4/2024-05-28'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the inquirer\n",
    "inquirer = StructRAGInquirer(\n",
    "    path_to_experiment=\"../results/\"+graph_path,\n",
    "    llm_name='google/flan-t5-large',\n",
    "    llm_max_tokens=512,\n",
    "    use_anchor_document=False # False means we compare against all the embeddings in the entire index, then find the context using the graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answers and context\n",
    "result_holder = []\n",
    "for tc in test_cases[\"test_cases\"]:\n",
    "    query = tc[\"question\"]\n",
    "\n",
    "    res = inquirer.run_inquirer(\n",
    "        query=query,\n",
    "        source_document_name=None,\n",
    "        k_context=5,\n",
    "    )\n",
    "\n",
    "    result_holder.append(res)\n",
    "\n",
    "# Save the results\n",
    "total_results[\"v4\"] = result_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = '/v5/2024-05-19'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the inquirer\n",
    "inquirer = StructRAGInquirer(\n",
    "    path_to_experiment=\"../results/\"+graph_path,\n",
    "    llm_name='google/flan-t5-large',\n",
    "    llm_max_tokens=512,\n",
    "    use_anchor_document=False # False means we compare against all the embeddings in the entire index, then find the context using the graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answers and context\n",
    "result_holder = []\n",
    "for tc in test_cases[\"test_cases\"]:\n",
    "    query = tc[\"question\"]\n",
    "\n",
    "    res = inquirer.run_inquirer(\n",
    "        query=query,\n",
    "        source_document_name=None,\n",
    "        k_context=5,\n",
    "    )\n",
    "\n",
    "    result_holder.append(res)\n",
    "\n",
    "# Save the results\n",
    "total_results[\"v5\"] = result_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_results.pkl', 'wb') as f:\n",
    "    pickle.dump(total_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate synthetic QA over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import pickle\n",
    "import pathlib\n",
    "import random\n",
    "from transformers import pipeline\n",
    "from llama_cpp import Llama\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukasalemu/Downloads/ls/envs/dissertation_rag/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "path_to_data = pathlib.Path(\"/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag/results/v0/2024-06-14/embedded_index.pickle\")\n",
    "with open(path_to_data, \"rb\") as f:\n",
    "    document_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id_', 'embedding', 'metadata', 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys', 'relationships', 'text', 'start_char_idx', 'end_char_idx', 'text_template', 'metadata_template', 'metadata_seperator', 'class_name'])\n"
     ]
    }
   ],
   "source": [
    "print(document_index[0].dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup a text-generation pipeline for the QA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install / activate the model\n",
    "path_to_save = pathlib.Path(\"/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag\")\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\n",
    "    filename=\"capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\",\n",
    "    local_dir=path_to_save,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-cd7f6d62-b261-495e-b8a8-b66ce8dba19a', 'object': 'text_completion', 'created': 1721082073, 'model': '/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag/capybarahermes-2.5-mistral-7b.Q5_K_M.gguf', 'choices': [{'text': 'Q: Name the planets in the solar system? A: 8 planets. Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 14, 'completion_tokens': 28, 'total_tokens': 42}}\n"
     ]
    }
   ],
   "source": [
    "output = llm(\n",
    "      \"Q: Name the planets in the solar system? A: \", # Prompt\n",
    "      max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=True # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b410118106a64287b71e1f4eb55ed653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_outputs = []\n",
    "outputs = []\n",
    "for sample_doc in tqdm(random.sample(document_index, 10)):\n",
    "    \n",
    "    context_text = sample_doc.text\n",
    "    context_text = context_text.replace(\"\\n\", \" \")\n",
    "    context_text = context_text.replace(\"  \", \" \")\n",
    "    \n",
    "    output = llm(\n",
    "        prompt=QA_generation_prompt.format(context=sample_doc.text),\n",
    "        max_tokens=None,\n",
    "        echo=True\n",
    "    )\n",
    "    raw_outputs.append(output)\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"context\": sample_doc.text,\n",
    "            \"question\": output['choices'][0]['text'].split(\"Output:::\")[-1].split('\\n')[1],\n",
    "            \"answer\": output['choices'][0]['text'].split(\"Output:::\")[-1].split('\\n')[-1],\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'Some contacts anticipate a pickup in transactional activity from 2024 Q2 onwards as funding\\ncosts stabilise or reduce. Others, especially those serving the construction and property\\nsectors, are more cautious and expect some recovery towards the end of 2024 or early 2025.\\nDomestic demand has softened for construction products and consumer goods. Food and\\ndrink output remains stable, but consumers continue to switch to cheaper brands and\\nproducts. High-tech sectors such as aerospace, defence, specialised capital and sustainable\\nequipment report activity continuing to pick up. Some contacts’ output was also supported by\\nbetter-performing export markets such as the US and Asia. Vehicle output improved as supply\\nchain disruption eased and demand was resilient.\\nFood, drink and consumer goods producers expect some recovery in demand later in the year .\\nHouse building, which remains the weakest sector, has slowed markedly over the past year\\ndue to weak demand and rising cost pressures, and is expected to soften further in the near\\nterm.\\nCommercial development has slowed more modestly, with increasing reports of public and\\ncommercial projects facing cancellations or delays. However, existing large infrastructure\\nprojects remain stable. Delays in construction schedules persist due to construction firm\\ninsolvencies, while planning and utility connections',\n",
       "  'question': 'Factoid question: What sectors report activity continuing to pick up according to the context?',\n",
       "  'answer': 'Answer: High-tech sectors such as aerospace, defence, specialised capital and sustainable equipment.'},\n",
       " {'context': 'or ‘reserves’, placed with the Bank o f England \\n–this is Bank  Rate. Second, we can buy government and corporate bonds, financed by the\\nissuance of central bank reserves – this is asset purchases or quantitative easing.  \\nThe Monetary Policy Report  \\nThe MPC is committed to clear, transparent comm unication. The Monetary Policy Report \\n(MPR) is a key part of that. It allows the MPC to share its thinking and explain the reasons for \\nits decisions.  \\nThe Report is produced quarterly by Bank staff under the guidance of the members of the \\nMPC.  \\nThis Report has been prepared and published by the Bank of England in accordance with \\nsection 18 of the Bank of England Act 1998.  \\nBank of England  \\nPage 1',\n",
       "  'question': 'Factoid question: What are the two ways through which a central bank can influence monetary policy?',\n",
       "  'answer': 'Answer: The two ways through which a central bank can influence monetary policy are by setting the Bank Rate and by purchasing government and corporate bonds through asset purchases or quantitative easing.'},\n",
       " {'context': 'its latest supply assessment the MPC has reduced its assumption\\nabout the size and persistence of the drag from Covid on the potential participation rate.\\nChart 3.2: The measured participation rate has risen since the previous stocktake\\nLabour force participation rate (a)\\nSource: ONS.\\n(a) Quarterly data to 2023 Q3 based on those aged 16+. The LFS series shown here uses official LFS estimates to 2023 Q2 and the\\nONS’s experimental alternative labour market statistics for Q3. See Box B of the November 2023 Report  for further information. Stalk\\nrepresents data that were available at the time of the previous supply assessment, which was conducted in February 2023.\\nPage 74\\nBank of England',\n",
       "  'question': 'Factoid question: What did the MPC reduce its assumption about in its latest supply assessment?',\n",
       "  'answer': 'Answer: the size and persistence of the drag from Covid on the potential participation rate'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assess the quality of the QA pairs\n",
    "\n",
    "Based on the logic in this paper arxiv2312.10003 [https://arxiv.org/abs/2312.10003](https://arxiv.org/abs/2312.10003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to macroeconomists working at the Bank of England.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c7c0e52ddd461482804841aa615017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_evals = []\n",
    "for output_bundle in tqdm(outputs):\n",
    "    \n",
    "    groundedness_eval = llm(\n",
    "        prompt=question_groundedness_critique_prompt.format(\n",
    "            question=output_bundle['question'],\n",
    "            context=output_bundle['context']\n",
    "        ),\n",
    "        max_tokens=None,\n",
    "        echo=True\n",
    "    )\n",
    "    \n",
    "    relevance_eval = llm(\n",
    "        prompt=question_relevance_critique_prompt.format(\n",
    "            question=output_bundle['question']\n",
    "        ),\n",
    "        max_tokens=None,\n",
    "        echo=True\n",
    "    )\n",
    "    \n",
    "    standalone_eval = llm(\n",
    "        prompt=question_standalone_critique_prompt.format(\n",
    "            question=output_bundle['question']\n",
    "        ),\n",
    "        max_tokens=None,\n",
    "        echo=True\n",
    "    )\n",
    "\n",
    "    # Extract the scores and write them\n",
    "    for eval_type, eval_output in zip([\"groundedness\", \"relevance\", \"standalone\"], [groundedness_eval, relevance_eval, standalone_eval]):\n",
    "        # If the model has stopped generating text due to reaching the max token limit\n",
    "        if eval_output['choices'][0][\"finish_reason\"] == 'stop':\n",
    "            output_bundle.update(\n",
    "                {\n",
    "                    f\"{eval_type}_score\": eval_output['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "                    f\"{eval_type}_rationale\": eval_output['choices'][0]['text'].split(\"Evaluation:\")[-1].strip()\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            output_bundle.update(\n",
    "                {\n",
    "                    f\"{eval_type}_score\": \"0\"\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # output_bundle.update(\n",
    "    #     {\n",
    "    #         \"groundedness_score\": groundedness_eval['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "    #         \"relevance_score\": relevance_eval['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "    #         \"standalone_score\": standalone_eval['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "    #     }\n",
    "    # )    \n",
    "    \n",
    "    raw_evals.append(\n",
    "        {\n",
    "            \"groundedness\": groundedness_eval,\n",
    "            \"relevance\": relevance_eval,\n",
    "            \"standalone\": standalone_eval\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The context clearly provides the answer to the question by stating \"The CPI measure'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_evals[7]['groundedness']['choices'][0]['text'].split('Evaluation:')[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any bad outputs using the scores - simple lambda\n",
    "filtered_outputs = list(filter(lambda x: x['groundedness_score'] >= 3 and x['relevance_score'] >= 3 and x['standalone_score'] >= 3, outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Despite the headwinds to income growth, households are relatively optimistic about their\\nfuture finances. In the Bank’s NMG survey, the measure of households’ expectations for\\ntheir own financial situation over the next year has improved substantially since 2022 and\\nis now in line with results prior to the pandemic. Survey responses also suggest that\\nhouseholds’ perceived risk of job loss has been falling and is now at its lowest level since\\n2015, although expectations for the level of economy-wide unemployment have increased\\nslightly over the past six months. The NMG’s measure of household income expectations\\nhas also risen, although this largely reflects the expectation that nominal incomes will\\ngrow given high inflation.\\nDuring the pandemic, household consumption fell by more than income as households\\nwere less able to spend on services, which meant that in aggregate households built up\\nadditional savings. Much of these additional savings took the form of bank deposits. As\\nshown on the left of Chart 3.4, the total stock of household deposits rose materially ,\\npeaking around 10% higher than its previous trend in 2022 Q1. A similar pattern has been\\nobserved across advanced economies (IMF (2023) ).…although households in the NMG survey remain',\n",
       " 'question': 'Factoid question: By how much did the total stock of household deposits rise peak compared to its previous trend in 2022 Q1?',\n",
       " 'answer': 'Answer: 10% higher than its previous trend',\n",
       " 'groundedness_score': '0',\n",
       " 'relevance_score': '5',\n",
       " 'standalone_score': '3'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

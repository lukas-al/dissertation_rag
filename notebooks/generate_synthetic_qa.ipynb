{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to generate synthetic QA over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import pickle\n",
    "import pathlib\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from transformers import pipeline\n",
    "from llama_cpp import Llama\n",
    "from pprint import pprint\n",
    "from importlib import reload\n",
    "\n",
    "# Add the project root directory to the system path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "PROJECT_ROOT = pathlib.Path(project_root)\n",
    "\n",
    "%aimport StructuredRag\n",
    "from StructuredRag.utils import mistral_conversation\n",
    "from StructuredRag.algorithms.inquirer import StructRAGInquirer\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'StructuredRag.algorithms.inquirer' from '/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag/src/StructuredRag/algorithms/inquirer.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(StructuredRag.algorithms.inquirer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load docs and instantiate our RAG and model agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = 'v0/2024-07-18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_data = pathlib.Path(\"/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag/results/v0/2024-06-14/embedded_index.pickle\")\n",
    "with open(PROJECT_ROOT / \"results\" / experiment_path / 'embedded_index.pickle', \"rb\") as f:\n",
    "    document_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading item: embedded_index\n",
      "Loading item: notes\n",
      "Loading item: edge_thresh\n",
      "Loading item: adj_matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding edges to graph: 100%|██████████| 440/440 [00:00<00:00, 97909.37it/s]\n"
     ]
    }
   ],
   "source": [
    "inquirer = StructuredRag.algorithms.inquirer.StructRAGInquirer(\n",
    "    path_to_experiment= str(PROJECT_ROOT / \"results\" / experiment_path),\n",
    "    llm_type='llamacpp',\n",
    "    model_path=str(PROJECT_ROOT / \"capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\"),\n",
    "    llm_max_tokens=1400,\n",
    "    n_gpu_layers=-1, # All layers\n",
    "    use_anchor_document=False,\n",
    ")\n",
    "\n",
    "# Use inquirer.llm for the llamacpp object\n",
    "# Otherwise use inquirer.run_inquirer for the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiated so we can use it to retreive the approapriate context\n",
    "# rag_agent = StructRAGInquirer(\n",
    "#     str((PROJECT_ROOT / \"results\" / 'v0' / '2024-07-18').resolve()),\n",
    "#     llm_name='google/flan-t5-large',\n",
    "#     llm_max_tokens=512,\n",
    "#     use_anchor_document=False,\n",
    "# )\n",
    "\n",
    "# result = rag_agent.run_inquirer(\n",
    "#     query = outputs[0]['question'].replace('Factoid question: ', ''),\n",
    "#     source_document_name=None,\n",
    "#     k_context = 3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context item 0: Committee continues to judge that the risks to its modal inflation projection are skewed to the upside. Second-round effects in domestic prices and wages are expected to take longer to unwind than they did to emerge. There are also upside risks to inflation from energy prices given events in the Middle East. Taking account of this skew, the mean projection for CPI inflation is 2.2% and 1.9% at the two and three-year horizons respectively. Conditioned on the alternative assumption of constant interest rates at 5.25%, which is a higher profile than the market curve beyond the second half of 2024, mean CPI inflation returns to target in two years’ time and falls to 1.6% at the three-year horizon. The MPC’s remit is clear that the inflation target applies at all times, reflecting the primacy of price stability in the UK monetary policy framework. The framework recognises that there will be occasions when inflation will depart from the target as a result of shocks and disturbances. Monetary policy will ensure that CPI inflation returns to the 2% target sustainably in the medium term. Since the MPC’s previous decision, there has been little news in key indicators of \n",
      "Context item 1: wholesale prices, the drag from utilities price inflation is then expected to fade from the second half of this year . Retail fuel prices fell earlier than utilities prices, and therefore the related drag has largely come through in the latest CPI data. The contribution of retail fuel prices to inflation is projected to be closer to zero in 2024 Q1 and positive in Q2, although that relies on the Government choosing to go ahead with theChart 2.18: Consumer price inflation has fallen since last year’s peak and is projected to fall further before rising again in 2024 H2 Contributions to CPI inflation (a) Sources: Bloomberg Finance L.P., Department for Energy Security and Net Zero, ONS and Bank calculations. (a) Figures in parentheses are CPI basket weights in 2023. Data to December 2023. Component-level Bank staf f projections from January 2024 to June 2024. Diamonds indicate projections for headline inflation in 2024 Q3 and Q4. The food component is defined as food and non-alcoholic beverages (FNAB). Fuels and lubricants estimates use Department for Energy Security and Net Zero petrol price data for January 2024 and then are based on the sterling oil futures \n",
      "Context item 2: declined to 6.5% in the three months to November, around 1 percentage point below the expectation in the November Report. This has brought the AWE measure back into line with the steer from other indicators of annual pay growth, which lies between 6% and 7% (Chart 2.15). Recent outturns in wage growth have also continued to be stronger than standard models of wage growth, based on productivity, short-term inflation expectations and a measure of economic slack, would have predicted (Chart 1.4). The MPC continues to judge that private sector regular AWE growth will ease more slowly than the range of forecasts from this suite of models would predict, consistent with its broader judgement that second-round effects in both domestic prices and wages will take longer to unwind than they did to emerge. Overall, the near-term outlook for pay growth is weaker than projected in the November Report, with the annual growth rate of private sector regular AWE projected to decline to around 4¾% by 2024 Q2 and to end this year at a similar rate. However, this profile also reflects an MPC judgement to push up slightly on pay growth in the near term, in light of the stronger forward-looking indications \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_context_for_QA_gen(doc, rag_agent, k_context: int = 3):\n",
    "    \"\"\"\n",
    "    Builds the context string for generating synthetic question-answering pairs.\n",
    "\n",
    "    Args:\n",
    "        doc: The document for which the context is being built.\n",
    "        rag_agent: The RAG agent used for retrieving similar nodes.\n",
    "        k_context (int): The number of similar nodes to consider for building the context. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        context_string (str): The generated context string containing the clean text of similar nodes.\n",
    "    \"\"\"\n",
    "    similar_nodes = rag_agent._graph_similar_nodes(doc.id_, k_context)\n",
    "    \n",
    "    context_string = \"\"\" \"\"\"\n",
    "    for i, (node_id, _) in enumerate(similar_nodes):\n",
    "        # Yes its unoptimised... find the document who's id matches the node_id\n",
    "        node = next((x for x in document_index if x.id_ == node_id), None)\n",
    "        \n",
    "        clean_text = node.text.replace(\"\\n\", \" \").replace(\"\\t\", \" \").replace(\"  \", \" \").strip()\n",
    "        context_string += f\"Context item {i}: {clean_text} \\n\"\n",
    "    \n",
    "    return context_string\n",
    "\n",
    "context_bundle = build_context_for_QA_gen(document_index[10], inquirer, k_context=3)\n",
    "\n",
    "print(context_bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup a text-generation pipeline for the QA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install / activate the model\n",
    "# path_to_save = pathlib.Path(\"/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag\")\n",
    "\n",
    "# llm = Llama.from_pretrained(\n",
    "#     repo_id=\"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\n",
    "#     filename=\"capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\",\n",
    "#     local_dir=PROJECT_ROOT,\n",
    "#     verbose=False,\n",
    "#     n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "#     n_ctx=1400,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "# Shortened and fitting into the CHATML format\n",
    "QA_SYSTEM_PROMPT = \"\"\" \n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\"\"\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chatML_QA_prompt(document_text: str) -> List[dict]: \n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": QA_SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": document_text,\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_text = document_index[100].text.replace(\"\\n\", \" \")\n",
    "\n",
    "# test = inquirer.llm.create_chat_completion(\n",
    "#     messages = create_chatML_QA_prompt(doc_text),\n",
    "# )\n",
    "\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0b38a40119440db4421ddff3667b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating QA pairs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the loop\n",
    "raw_outputs = []\n",
    "outputs = []\n",
    "for sample_doc in tqdm(random.sample(document_index, 10), desc=\"Generating QA pairs\"):\n",
    "    \n",
    "    context_text = build_context_for_QA_gen(sample_doc, inquirer, k_context=3)\n",
    "    \n",
    "    output = inquirer.llm.create_chat_completion(messages = create_chatML_QA_prompt(context_text))\n",
    "    \n",
    "    raw_outputs.append(output)\n",
    "    outputs.append(\n",
    "        {\n",
    "            \"context\": context_text,\n",
    "            \"question\": output['choices'][0]['message']['content'].split('Factoid question: ')[1].split('\\n')[0],\n",
    "            'answer': output['choices'][0]['message']['content'].split('Answer: ')[1],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-edab8a0d-89b9-4944-8cb8-932cf58f40ec',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1721393338,\n",
       " 'model': '/Users/lukasalemu/Documents/00. Bank of England/00. Degree/Dissertation/structured-rag/capybarahermes-2.5-mistral-7b.Q5_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'Factoid question: How quickly has core goods inflation fallen compared to services inflation across advanced economies?\\n\\nAnswer: Core goods inflation has fallen more quickly than services inflation.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 812, 'completion_tokens': 36, 'total_tokens': 848}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_outputs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### OLD PRE FORMATTED PROMPT\n",
    "# raw_outputs = []\n",
    "# outputs = []\n",
    "# for sample_doc in tqdm(random.sample(document_index, 10)):\n",
    "    \n",
    "#     context_text = sample_doc.text\n",
    "#     context_text = context_text.replace(\"\\n\", \" \")\n",
    "#     context_text = context_text.replace(\"  \", \" \")\n",
    "    \n",
    "#     output = llm(\n",
    "#         prompt=QA_generation_prompt.format(context=context_text),\n",
    "#         max_tokens=None,\n",
    "#         echo=True\n",
    "#     )\n",
    "#     raw_outputs.append(output)\n",
    "#     outputs.append(\n",
    "#         {\n",
    "#             \"context\": context_text,\n",
    "#             \"question\": output['choices'][0]['text'].split(\"Output:::\")[-1].split('\\n')[1],\n",
    "#             \"answer\": output['choices'][0]['text'].split(\"Output:::\")[-1].split('\\n')[-1],\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Assess the quality of the QA pairs\n",
    "\n",
    "Based on the logic in this paper arxiv2312.10003 [https://arxiv.org/abs/2312.10003](https://arxiv.org/abs/2312.10003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_GROUNDEDNESS_PROMPT = \"\"\"\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the context and question.\n",
    "\"\"\"\n",
    "\n",
    "QA_RELEVANCE_PROMPT = \"\"\"\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to macro-economists looking for information whilst working at the Bank of England.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\"\"\"\n",
    "\n",
    "QA_STANDALONE_PROMPT = \"\"\"\n",
    "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like MPC, CPI or YBUS and still be a 5.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chatML_quality_prompt(prompt: str, question: str, context: str = None) -> List[dict]:\n",
    "    if context is None:\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Factoid question: {question}\",\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {context}\\nFactoid question: {question}\",\n",
    "            },\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_bundle = outputs[4]\n",
    "\n",
    "# groundedness_eval = inquirer.llm.create_chat_completion(\n",
    "#         messages = create_chatML_quality_prompt(QA_GROUNDEDNESS_PROMPT, output_bundle['question'], output_bundle['context']),\n",
    "#     )\n",
    "\n",
    "# groundedness_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7993b368ef43bc93becdf52dbc6f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating QA pairs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_evals = []\n",
    "for output_bundle in tqdm(outputs, desc=\"Evaluating QA pairs\"):\n",
    "    \n",
    "    groundedness_eval = inquirer.llm.create_chat_completion(\n",
    "        messages = create_chatML_quality_prompt(QA_GROUNDEDNESS_PROMPT, output_bundle['question'], output_bundle['context']),\n",
    "    )\n",
    "    \n",
    "    relevance_eval = inquirer.llm.create_chat_completion(\n",
    "        messages = create_chatML_quality_prompt(QA_RELEVANCE_PROMPT, output_bundle['question']),\n",
    "    )\n",
    "    \n",
    "    standalone_eval = inquirer.llm.create_chat_completion(\n",
    "        messages = create_chatML_quality_prompt(QA_STANDALONE_PROMPT, output_bundle['question']),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Extract the scores and write them\n",
    "    for eval_type, eval_output in zip([\"groundedness\", \"relevance\", \"standalone\"], [groundedness_eval, relevance_eval, standalone_eval]):\n",
    "        # If the model has stopped generating text due to stopping itself, rather than max tokens, etc.\n",
    "        if eval_output['choices'][0][\"finish_reason\"] == 'stop':\n",
    "            output_bundle.update(\n",
    "                {\n",
    "                    f\"{eval_type}_score\": eval_output['choices'][0]['message']['content'].split('Total rating: ')[1].split('\\n')[0].strip(),\n",
    "                    f\"{eval_type}_rationale\": eval_output['choices'][0]['message']['content'].split('Evaluation: ')[1].split('\\n')[0].strip(),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            output_bundle.update(\n",
    "                {\n",
    "                    f\"{eval_type}_score\": \"0\",\n",
    "                    f\"{eval_type}_rationale\": \"\"\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    raw_evals.append(\n",
    "        {\n",
    "            \"groundedness\": groundedness_eval,\n",
    "            \"relevance\": relevance_eval,\n",
    "            \"standalone\": standalone_eval\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ' Context item 0: to 7.4% in July but declined to 6.8% in August, 0.3 percentage points lower than expected in the August Report. Some of those movements were linked to services such as airfares and accommodation that tend to be volatile over the summer holiday period. Excluding these travel-related components, services inflation had been more stable at continued high rates, albeit slightly weaker than expected. CPI inflation was expected to fall significantly further in the near term, reflecting lower annual energy inflation, despite the renewed upward pressure from oil prices, and further declines in food and core goods price inflation. Services price inflation, however, was projected to remain elevated in the near term, with some potential month-to-month volatility. Developments in key indicators of inflation persistence had been mixed, with the acceleration in the AWE not apparent in other measures of wages and with some downside news on services inflation. There were increasing signs of some impact of tighter monetary policy on the labour market and on momentum in the real economy more generally. Given the significant increase in Bank Rate since the start of this tightening cycle, the current monetary policy stance was restrictive. The MPC would continue to monitor closely indications of persistent \\n',\n",
       " 'question': 'What was the change in CPI inflation between July and August?',\n",
       " 'answer': 'CPI inflation declined from 7.4% in July to 6.8% in August, which is 0.3 percentage points lower than expected in the August Report.',\n",
       " 'groundedness_score': '5',\n",
       " 'groundedness_rationale': 'The context clearly states that \"to 7.4% in July but declined to 6.8% in August,\" which directly answers the question about the change in CPI inflation between those two months.',\n",
       " 'relevance_score': '3',\n",
       " 'relevance_rationale': 'This question is useful for macro-economists working at the Bank of England because it requires them to stay updated with current inflation data. However, the usefulness depends on whether the information is relevant to their specific area of study or research. The question itself is factoid and requires a specific answer that may change over time.',\n",
       " 'standalone_score': '5',\n",
       " 'standalone_rationale': 'The question is clear and self-contained, making it context-independent. It only requires knowledge of the CPI inflation data for the specified time period.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out any bad outputs using the scores - simple lambda\n",
    "filtered_outputs = list(filter(lambda x: float(x['groundedness_score']) >= 2 and float(x['relevance_score']) >= 2 and float(x['standalone_score']) >= 2, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs), len(filtered_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_groundedness_critique_prompt = \"\"\"\n",
    "# Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here are the question and context.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Context: {context}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_relevance_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how useful this question can be to macro-economists looking for information at the Bank of England.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\"\n",
    "\n",
    "# question_standalone_critique_prompt = \"\"\"\n",
    "# You will be given a question.\n",
    "# Your task is to provide a 'total rating' representing how context-independent this question is.\n",
    "# Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "# For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "# The questions can contain obscure technical nouns or acronyms like MPC, Committee, CPI or YBUS and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "# For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "# Provide your answer as follows:\n",
    "\n",
    "# Answer:::\n",
    "# Evaluation: (your rationale for the rating, as a text)\n",
    "# Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "# You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "# Now here is the question.\n",
    "\n",
    "# Question: {question}\\n\n",
    "# Answer::: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_evals = []\n",
    "# for output_bundle in tqdm(outputs, desc=\"Evaluating QA pairs\"):\n",
    "    \n",
    "#     groundedness_eval = llm(\n",
    "#         prompt=question_groundedness_critique_prompt.format(\n",
    "#             question=output_bundle['question'],\n",
    "#             context=output_bundle['context']\n",
    "#         ),\n",
    "#         max_tokens=None,\n",
    "#         echo=True\n",
    "#     )\n",
    "    \n",
    "#     relevance_eval = llm(\n",
    "#         prompt=question_relevance_critique_prompt.format(\n",
    "#             question=output_bundle['question']\n",
    "#         ),\n",
    "#         max_tokens=None,\n",
    "#         echo=True\n",
    "#     )\n",
    "    \n",
    "#     standalone_eval = llm(\n",
    "#         prompt=question_standalone_critique_prompt.format(\n",
    "#             question=output_bundle['question']\n",
    "#         ),\n",
    "#         max_tokens=None,\n",
    "#         echo=True\n",
    "#     )\n",
    "\n",
    "#     # Extract the scores and write them\n",
    "#     for eval_type, eval_output in zip([\"groundedness\", \"relevance\", \"standalone\"], [groundedness_eval, relevance_eval, standalone_eval]):\n",
    "#         # If the model has stopped generating text due to stopping itself, rather than max tokens, etc.\n",
    "#         if eval_output['choices'][0][\"finish_reason\"] == 'stop':\n",
    "#             output_bundle.update(\n",
    "#                 {\n",
    "#                     f\"{eval_type}_score\": eval_output['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "#                     f\"{eval_type}_rationale\": eval_output['choices'][0]['text'].split(\"Evaluation:\")[-1].strip()\n",
    "#                 }\n",
    "#             )\n",
    "#         else:\n",
    "#             output_bundle.update(\n",
    "#                 {\n",
    "#                     f\"{eval_type}_score\": \"0\",\n",
    "#                     f\"{eval_type}_rationale\": \"\"\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     # output_bundle.update(\n",
    "#     #     {\n",
    "#     #         \"groundedness_score\": groundedness_eval['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "#     #         \"relevance_score\": relevance_eval['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "#     #         \"standalone_score\": standalone_eval['choices'][0]['text'].split(\"Total rating:\")[-1].strip(),\n",
    "#     #     }\n",
    "#     # )    \n",
    "    \n",
    "#     raw_evals.append(\n",
    "#         {\n",
    "#             \"groundedness\": groundedness_eval,\n",
    "#             \"relevance\": relevance_eval,\n",
    "#             \"standalone\": standalone_eval\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate answers to the Qs with the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ' Context item 0: Interest rate rises will also reduce non-housing asset prices. Indeed, net financial wealth relative to household incomes fell materially in 2022 despite no reduction in nominal household deposits (Broadbent (2022) ). However, changes in household financial wealth tend to have less overall impact on demand because most households do not own significant non-housing and non-pension wealth. The effect can be important for those households at the top of the wealth distribution where, prior to the pandemic, the richest tenth of households had 17% of their total net wealth in financial assets. The consumption effect of reductions in the value of financial wealth are captured within the purple bars in Chart 3.7. Although not directly affected by rising interest rates, households in the rental sector may also face increased housing costs, leading to further reductions in consumer demand. Specifically, rising interest rates increase costs for buy-to-let (BTL) landlords with a mortgage and reduce their returns through lower house prices. Landlords may try to pass on their costs through rent increases. In the long term, rent rises will be driven byChart 3.9: Some measures of nominal house prices have fallen somewhat in \\nContext item 1: to slow slightly next year but remain positive. Investment intentions are being held back by economic uncertainty, and by the cost and availability of finance, but supported by the need to invest for a number of reasons such as digitalisation, ef ficiency, sustainability and maintenance. Further out in the latest projection, business investment is expected to be broadly flat in 2025, before increasing by 2% in 2026. There are risks in both directions around the central projections for household spending and GDP, including related to the Committee’s decision in this forecast to scale back somewhat the extent of its judgement to boost expected demand. Spending could be stronger than expected if there is greater resilience in labour market activity (Key judgement 2) and some households choose to save less or run down existing stocks of savings to a greater extent. Conversely, demand could be weaker than expected if some people become more worried about their job security and try to build up their savings to a greater extent. The latest Bank/NMG survey suggests that, among those who are planning to change their saving habits compared with the previous six months, the share of households who are planning to save more than usual over the next six months \\nContext item 2: In the modal projection conditioned on the alternative assumption of constant interest rates at 5.25% over the forecast period, CPI inflation is expected to be around the 2% target during the first half of the forecast period before falling below it from 2025 Q4 onwards (Chart 1.6). This path is significantly lower than the Committee’s modal projection conditioned on market rates. There are near-term risks in both directions around the paths of CPI inflation and pay growth from domestic factors. On the one hand, a continuation of recent trends in AWE and services inflation could lead to a further faster-than-expected decline in domestic price pressures. The Bank’s Agents also report that, unlike last year, the vast majority of companies are not expecting to make one-off payments this year beyond regular bonuses. On the other hand, there remain upside risks to the outlook for wage growth, including from the possible effects of the increase in the National Living Wage and, more broadly, as the Agents’ annual pay survey suggests settlements in 2024 as a whole will moderate only slightly on average. Nevertheless, intelligence from the Agents suggests that companies will not be able to pass on increased labour costs into prices as much as they \\n',\n",
       " 'question': 'In 2022, what happened to net financial wealth relative to household incomes?',\n",
       " 'answer': 'Net financial wealth relative to household incomes fell materially in 2022.',\n",
       " 'groundedness_score': '5',\n",
       " 'groundedness_rationale': 'The context provides information about the change in net financial wealth relative to household incomes in 2022. It states that \"net financial wealth relative to household incomes fell materially in 2022 despite no reduction in nominal household deposits.\" This statement directly answers the question.',\n",
       " 'relevance_score': '3',\n",
       " 'relevance_rationale': 'This question is relevant to macro-economists as it relates to household net financial wealth, which can be an important indicator of economic stability and consumer spending. Understanding changes in this ratio over time can provide insights into the distribution of wealth and potential future economic trends. The year 2022 is specific enough to prompt a focused analysis, but the lack of context makes it difficult to determine the exact usefulness without additional information about the source or purpose of the question.',\n",
       " 'standalone_score': '5',\n",
       " 'standalone_rationale': 'This question is context-independent as it refers to a factual event that can be answered with a specific piece of information.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a2f63983514603b20cee0e35a9f7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running inquirer:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for qa_bundle in tqdm(filtered_outputs, desc=\"Running inquirer\"):\n",
    "    response = inquirer.run_inquirer(\n",
    "        query = qa_bundle['question'],\n",
    "        source_document_name=None,\n",
    "        k_context = 3,\n",
    "    )\n",
    "    \n",
    "    qa_bundle.update(\n",
    "        {\n",
    "            \"RAG_response\": response,\n",
    "            \"RAG_response_text\": response['choices'][0]['message']['content'],\n",
    "        }\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The expected growth rate of private sector regular AWE by the end of the MPC\\'s modal projection is around 3%. This information can be found in Context 0, which states that \"private sector regular AWE growth falls to around 3% by the end of the forecast period.\"'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use the filtered QA pairs and RAG answers to assess the RAG system, with a 'judge' agent.\n",
    "\n",
    "There are different types of [evaluation metrics in RAG](https://docs.ragas.io/en/latest/concepts/metrics/index.html)\n",
    "\n",
    "We will focus on:\n",
    "1. Answer relevancy\n",
    "2. Faithfullness\n",
    "\n",
    "We will use https://huggingface.co/prometheus-eval/prometheus-13b-v1.0 as our evaluation model, or something similar. Found [a GGUF vsevolodl/prometheus-7b-v2.0-GGUF](https://huggingface.co/vsevolodl/prometheus-7b-v2.0-GGUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm = Llama.from_pretrained(\n",
    "    repo_id=\"vsevolodl/prometheus-7b-v2.0-GGUF\",\n",
    "    filename=\"prometheus-7b-v2.0.Q6_K.gguf\",\n",
    "    local_dir=PROJECT_ROOT,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1, # Uncomment to use GPU acceleration\n",
    "    n_ctx=1600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction wit\n",
    "judge_prompt = \"\"\"\n",
    "###Task Description:\n",
    "An instruction (including the context) a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "You are an AI assistant with a focus on helping to answer economists' search questions over particular documents. \n",
    "Respond only to the question asked, the response should be concise and relevant, and use the context provided to give a comprehensive answer.\n",
    "It is important to maintain impartiality and non-partisanship. If you are unable to answer a question based on the given instructions and context, please indicate so.\n",
    "Your responses should be well-structured and professional, using British English.\n",
    "\n",
    "{query} Use the following context to answer the question:\n",
    "Context: {context}\n",
    "\n",
    "###Response to evaluate:\n",
    "{answer_RAG_system}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback: \n",
    "\"\"\"\n",
    "\n",
    "conv = mistral_conversation.get_conv_template(\"mistral\")\n",
    "conv.set_system_message(\"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\")\n",
    "conv.append_message(conv.roles[0], judge_prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "\n",
    "judge_prompt = conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144cf93367d04a0293282d230b088561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running judgement llm:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for qa_bundle in tqdm(filtered_outputs, desc=\"Running judgement llm\"):\n",
    "    evaluation = judge_llm.create_completion(\n",
    "        prompt=judge_prompt.format(\n",
    "            query=qa_bundle['question'],\n",
    "            context=qa_bundle['context'],\n",
    "            reference_answer=qa_bundle['answer'],\n",
    "            answer_RAG_system=qa_bundle['RAG_response']['choices'][0]['message']['content'],\n",
    "        ),\n",
    "        echo=True,\n",
    "        max_tokens=None\n",
    "    )\n",
    "    \n",
    "    qa_bundle.update(\n",
    "        {\n",
    "            \"judge_evaluation\": evaluation,\n",
    "            \"judge_score\": evaluation['choices'][0]['text'].split('[RESULT]')[-1].strip()\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. View and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'What was the change in CPI inflation between July and August?'\n",
      "('Based on the provided context, we can find the change in CPI inflation '\n",
      " 'between July and August in Context 0:\\n'\n",
      " '\\n'\n",
      " 'Context 0 states that \"Core goods CPI inflation had fallen from 6.4% in June '\n",
      " 'to 5.2% in August.\" This indicates a decrease of 1.2 percentage points in '\n",
      " 'core goods CPI inflation between June and August. Additionally, it mentions '\n",
      " 'that Services CPI inflation rose from 7.2% in June to an unspecified value '\n",
      " 'in August.\\n'\n",
      " '\\n'\n",
      " 'However, the question asks for the change in overall CPI inflation between '\n",
      " 'July and August, which is not explicitly mentioned in any of the contexts '\n",
      " 'provided. If you could provide more specific information about the overall '\n",
      " 'CPI inflation rates for July and August, I would be able to calculate the '\n",
      " 'change accurately.')\n",
      "'1'\n",
      "('The response provided fails to directly answer the question about the change '\n",
      " 'in overall CPI inflation between July and August. While it correctly '\n",
      " 'identifies a decrease in core goods CPI inflation from June to August, it '\n",
      " 'does not provide any information regarding the change in overall CPI '\n",
      " 'inflation during the given period. This is because the context given does '\n",
      " 'not explicitly mention the overall CPI inflation rates for July and August. '\n",
      " 'In order to calculate the correct change, such specific figures are '\n",
      " 'required. The response also fails to recognize that there was an expected '\n",
      " 'drop in CPI inflation which was not met, and it does not address any '\n",
      " 'potential impacts of monetary policy on inflation. Therefore, based on the '\n",
      " 'score rubric which requires the response to be accurate, factual, and '\n",
      " 'directly answer the question asked, this response is largely incorrect and '\n",
      " 'inaccurate. It does not provide a comprehensive or professional answer using '\n",
      " 'British English. The response would benefit from direct reference to '\n",
      " 'specific CPI inflation figures for July and August and an analysis of how '\n",
      " 'these figures relate to overall inflation rates.')\n"
     ]
    }
   ],
   "source": [
    "pprint(filtered_outputs[1]['question'])\n",
    "pprint(filtered_outputs[1]['RAG_response']['choices'][0]['message']['content'])\n",
    "pprint(filtered_outputs[1]['judge_score'])\n",
    "pprint(filtered_outputs[1]['judge_evaluation']['choices'][0]['text'].split('[/INST]')[1].split('[RESULT]')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outputs\n",
    "with open(PROJECT_ROOT / \"results\" / experiment_path / 'QA_outputs.pickle', \"wb\") as f:\n",
    "    pickle.dump(filtered_outputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

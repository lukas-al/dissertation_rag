{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev 2 Boogaloo\n",
    "\n",
    "What to do:\n",
    "- [] Add some more targeted 'needle in a haystack' test cases\n",
    "- [] Replace the FLAN model for the AG stage\n",
    "- [x] Replace the embedding model with a better one\n",
    "- [] Pre-clean some of the text, \\n, multiple numbers, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not load model llmware/dragon-yi-6b-v0 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 558, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3451, in from_pretrained\n    raise EnvironmentError(\nOSError: llmware/dragon-yi-6b-v0 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mn:\\CECD\\10. Personal\\Lukas Alemu\\Study Repository\\99. Capstone\\dissertation_rag\\notebooks\\dev2_boogaloo.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/resolve/main/capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# pipe = pipeline(\"text-generation\", model=\"TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q6_K.gguf\")\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mllmware/dragon-yi-6b-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\pipelines\\__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    905\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 906\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    907\u001b[0m         model,\n\u001b[0;32m    908\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m    909\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    910\u001b[0m         framework\u001b[39m=\u001b[39mframework,\n\u001b[0;32m    911\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[0;32m    912\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[0;32m    913\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    916\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    917\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\pipelines\\base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[39mfor\u001b[39;00m class_name, trace \u001b[39min\u001b[39;00m all_traceback\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    295\u001b[0m             error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhile loading with \u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m, an error is thrown:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtrace\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 296\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    297\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m with any of the following classes: \u001b[39m\u001b[39m{\u001b[39;00mclass_tuple\u001b[39m}\u001b[39;00m\u001b[39m. See the original errors:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         )\n\u001b[0;32m    300\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     framework \u001b[39m=\u001b[39m infer_framework(model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model llmware/dragon-yi-6b-v0 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 558, in from_pretrained\n    return model_class.from_pretrained(\n  File \"c:\\Miniconda3\\envs\\diss_rag\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3451, in from_pretrained\n    raise EnvironmentError(\nOSError: llmware/dragon-yi-6b-v0 does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF/resolve/main/capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\n",
    "# pipe = pipeline(\"text-generation\", model=\"TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q6_K.gguf\")\n",
    "pipe = pipeline(\"text-generation\", model=\"llmware/dragon-yi-6b-v0\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Llama.__init__() missing 1 required positional argument: 'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\CECD\\10. Personal\\Lukas Alemu\\Study Repository\\99. Capstone\\dissertation_rag\\notebooks\\dev2_boogaloo.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m Llama(\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     repo_id \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mTheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     filename \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcapybarahermes-2.5-mistral-7b.Q5_K_M.gguf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     verbose \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/CECD/10.%20Personal/Lukas%20Alemu/Study%20Repository/99.%20Capstone/dissertation_rag/notebooks/dev2_boogaloo.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: Llama.__init__() missing 1 required positional argument: 'model_path'"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    repo_id = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\",\n",
    "    filename = \"capybarahermes-2.5-mistral-7b.Q5_K_M.gguf\",\n",
    "    verbose = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
